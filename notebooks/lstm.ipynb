{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "9b36c0ac-446b-45d2-b352-805236a41b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import lib\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Input, LSTM, Dense, BatchNormalization, Dropout\n",
    "from keras.regularizers import L1L2\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "sys.path.append(\"../scripts\")\n",
    "from data_loader import get_stocks, get_etfs, get_technical_indicators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f8ee0d14-2756-4f94-ad74-2bbd61f6295f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define global constant\n",
    "STOCKS = ['AAPL', 'MSFT', 'GOOGL']\n",
    "HORIZONS = [1, 3, 7, 30, 90] # 1 day, 3 days, 1 week, 1 month, 3 months "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "614f7664-610e-46c5-907e-365593ce8e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(stock, testing_size, seq_length=720, horizon=1, threshold=0.01): \n",
    "    # load the corresponding stock\n",
    "    df = get_stocks(stock)\n",
    "    df = get_technical_indicators(df)\n",
    "    df = df[len(df)-testing_size:]\n",
    "\n",
    "    # set up the data \n",
    "    df['Price_Change'] = (df['Close'].shift(-horizon) - df['Close']) / df['Close']\n",
    "    df['Target'] = (df['Price_Change'] > threshold).astype(int) | (df['Price_Change'] < -threshold).astype(int)\n",
    "    X = df.drop(columns=['Date', 'Stock', 'Target', 'Price_Change']) # drop the irrevalent variables for training set\n",
    "    y = df['Target'].values\n",
    "\n",
    "    # normalise the vector\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    X = scaler.fit_transform(X)\n",
    "    \n",
    "    return X, y, scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "38f585ed-e9f8-47eb-8cbd-3ad8e730c8a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sequences(X, y, seq_length):\n",
    "    \"\"\"Create properly aligned sequences where:\n",
    "    - Each X sequence contains seq_length past observations\n",
    "    - Each y sequence contains corresponding targets\n",
    "    - Both have exactly the same length\"\"\"\n",
    "    X_seq, y_seq = [], []\n",
    "    for i in range(seq_length, len(X)):\n",
    "            X_seq.append(X[i-seq_length:i])  # Past seq_length observations\n",
    "            y_seq.append(y[i])   # Corresponding targets\n",
    "    return np.array(X_seq), np.array(y_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "0946adbe-419c-42f0-8ea0-0927c604be74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build lstm model\n",
    "def get_enhanced_lstm(seq_len, num_features):\n",
    "    model = Sequential([\n",
    "        Input(shape=(seq_len, num_features)),\n",
    "        LSTM(128, return_sequences=True, \n",
    "             kernel_regularizer=L1L2(l1=1e-5, l2=1e-4),\n",
    "             recurrent_dropout=0.2),\n",
    "        BatchNormalization(),\n",
    "        LSTM(64, return_sequences=True,\n",
    "             kernel_regularizer=L1L2(l1=1e-5, l2=1e-4)),\n",
    "        Dropout(0.3),\n",
    "        LSTM(32),\n",
    "        BatchNormalization(),\n",
    "        Dense(32, activation='relu'),\n",
    "        Dropout(0.2),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    \n",
    "    optimizer = Adam(learning_rate=0.001)\n",
    "    # model.compile(optimizer=optimizer, loss='mse', metrics=['mae'])\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "9b7c9890-11fb-4091-861f-4d08611d5d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_classification(y_true, y_pred_probs, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Compute comprehensive classification metrics\n",
    "    Args:\n",
    "        y_true: True binary labels (0 or 1)\n",
    "        y_pred_probs: Predicted probabilities (0 to 1)\n",
    "        threshold: Cutoff for binary classification\n",
    "    Returns:\n",
    "        Dictionary of metrics and plots\n",
    "    \"\"\"\n",
    "    # Convert probabilities to binary predictions\n",
    "    y_pred = (y_pred_probs >= threshold).astype(int)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    metrics = {\n",
    "        'accuracy': accuracy_score(y_true, y_pred),\n",
    "        'f1': f1_score(y_true, y_pred),\n",
    "        'roc_auc': roc_auc_score(y_true, y_pred_probs),\n",
    "    }\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "df86f7cf-4135-481b-a5ac-a063ea29beed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model by rolling window and predict the outcome\n",
    "def rolling_window_train(stock, horizon=1, train_ratio=0.8, window_size=720, epochs=5, batch_size=32, testing_size=1000):\n",
    "    predictions = []\n",
    "    actuals = []\n",
    "    probability_predictions = []\n",
    "    \n",
    "    # prepare data for the seq_length\n",
    "    X, y, scaler = prepare_data(stock, seq_length=window_size, testing_size=testing_size, horizon=horizon)\n",
    "    test_start = int(len(X)*train_ratio)\n",
    "    \n",
    "    # get sequence data\n",
    "    X, y = get_sequences(X, y, window_size)\n",
    "\n",
    "    i = test_start\n",
    "    while (i < len(X)):\n",
    "        i_seq = i-window_size\n",
    "        # reset the model\n",
    "        model = get_enhanced_lstm(window_size, X.shape[2])\n",
    "\n",
    "        # set up the variables for training\n",
    "        X_window, y_window = X[i_seq-window_size:i_seq], y[i_seq-window_size:i_seq]\n",
    "        steps = window_size // batch_size // epochs\n",
    "\n",
    "        # fit the model with rolling window\n",
    "        model.fit(X_window, y_window, epochs=epochs, batch_size=batch_size, steps_per_epoch=steps, verbose=0)\n",
    "\n",
    "        # Predict next value\n",
    "        pred = model.predict(X[i_seq:i_seq+1])\n",
    "        binary_pred = 1 if pred > 0.5 else 0\n",
    "\n",
    "        predictions.append(binary_pred)\n",
    "        actuals.append(y[i_seq])\n",
    "        probability_predictions.append(pred)\n",
    "\n",
    "        i += 1\n",
    "\n",
    "    return np.array(predictions).flatten(), np.array(actuals).flatten(), np.array(probability_predictions).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "fa30fa3c-0072-4bd9-8a2d-8c196e325213",
   "metadata": {},
   "outputs": [],
   "source": [
    "def expanding_window_train(stock, horizon, train_ratio=0.8, window_size=720, epochs=5, batch_size=32, testing_size=1000):\n",
    "    predictions = []\n",
    "    actuals = []\n",
    "    probability_predictions = []\n",
    "    X, y, scaler = prepare_data(stock, seq_length=window_size, testing_size=testing_size, horizon=horizon)\n",
    "    test_start = int(len(X)*train_ratio)\n",
    "\n",
    "    i = test_start\n",
    "    while (i < len(X)):\n",
    "        i_seq = i-window_size\n",
    "        X_seq, y_seq = get_sequences(X, y, window_size)\n",
    "        # reset the model\n",
    "        model = get_enhanced_lstm(window_size, X_seq.shape[2])\n",
    "\n",
    "        # set up the variables for training\n",
    "        X_window, y_window = X_seq[i_seq-window_size:i_seq], y_seq[i_seq-window_size:i_seq]\n",
    "        steps = window_size // batch_size // epochs\n",
    "\n",
    "        # fit the model with rolling window\n",
    "        model.fit(X_window, y_window, epochs=epochs, batch_size=batch_size, steps_per_epoch=steps, verbose=0)\n",
    "\n",
    "        # Predict next value\n",
    "        # print(X_seq[i_seq:i_seq+1].shape)\n",
    "        # pred = model.predict(X_seq[i_seq:i_seq+1].reshape(1, window_size, X_seq.shape[2]))\n",
    "        # pred = scaler.inverse_transform(pred[0,0].reshape(-1,1))\n",
    "        # predictions.append(pred)\n",
    "        # y_test_original = scaler.inverse_transform(y_seq[i_seq][0].reshape(-1,1))\n",
    "        # actuals.append(y_test_original)\n",
    "\n",
    "        pred = model.predict(X_seq[i_seq:i_seq+1])\n",
    "        binary_pred = 1 if pred > 0.5 else 0\n",
    "\n",
    "        predictions.append(binary_pred)\n",
    "        actuals.append(y_seq[i_seq])\n",
    "        probability_predictions.append(pred)\n",
    "\n",
    "        i += 1\n",
    "        window_size += 1\n",
    "\n",
    "    return np.array(predictions).flatten(), np.array(actuals).flatten(), np.array(probability_predictions).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "1bba318e-f080-42d6-906f-1201b3b3aa44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    print(\"training & predicting by roll window:\\n\")\n",
    "    roll_predictions, roll_actuals, roll_probability_predictions = rolling_window_train(STOCKS[0], HORIZONS[0], window_size=10, epochs=2, batch_size=5, testing_size=100)\n",
    "    print(\"training & predicting by expand window\\n\")\n",
    "    expand_predictions, expand_actuals, expand_probability_predictions = expanding_window_train(STOCKS[0], HORIZONS[0], window_size=10, epochs=2, batch_size=5, testing_size=100)\n",
    "    \n",
    "    # evaulate the model performance\n",
    "    roll_matrics = evaluate_classification(roll_actuals, roll_probability_predictions, threshold=0.5)\n",
    "    expand_matrics = evaluate_classification(expand_actuals, expand_probability_predictions, threshold=0.5)\n",
    "\n",
    "    print(f\"roll_matrics:\\n{roll_matrics}\\n\", f\"expand_matrics:\\n{expand_matrics}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "af12fba0-7189-401c-a966-173981d3330b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training & predicting by roll window:\n",
      "\n",
      "✅ Processed data already exists. Skipping processing.\n",
      "✅ Technical indicators added successfully.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 598ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 890ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 678ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 658ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 646ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 636ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 654ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 613ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 577ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 727ms/step\n",
      "training & predicting by expand window\n",
      "\n",
      "✅ Processed data already exists. Skipping processing.\n",
      "✅ Technical indicators added successfully.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 692ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 662ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 592ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 634ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 577ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 588ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 558ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 571ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 571ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 644ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 632ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 575ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 563ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 582ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 558ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 616ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 565ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 537ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 781ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step   \n",
      "roll_matrics:\n",
      "{'accuracy': 0.7, 'f1': 0.5714285714285714, 'roc_auc': 0.6190476190476191}\n",
      " expand_matrics:\n",
      "{'accuracy': 0.45, 'f1': 0.35294117647058826, 'roc_auc': 0.4583333333333333}\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "226fb1b3-b9d4-45cd-a404-9f56e25610eb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
