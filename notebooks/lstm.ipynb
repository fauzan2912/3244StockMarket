{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "9b36c0ac-446b-45d2-b352-805236a41b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import lib\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Input, LSTM, Dense, BatchNormalization, Dropout\n",
    "from keras.regularizers import L1L2\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "sys.path.append(\"../scripts\")\n",
    "from data_loader import get_stocks, get_etfs, get_technical_indicators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "f8ee0d14-2756-4f94-ad74-2bbd61f6295f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define global constant\n",
    "STOCKS = ['AAPL', 'MSFT', 'GOOGL']\n",
    "HORIZONS = [1, 3, 7, 30, 90] #Â 1 day, 3 days, 1 week, 1 month, 3 months "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "614f7664-610e-46c5-907e-365593ce8e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(stock, testing_size, seq_length=720, horizon=1): \n",
    "    # load the corresponding stock\n",
    "    df = get_stocks(stock)\n",
    "    df = get_technical_indicators(df)\n",
    "    df = df[len(df)-testing_size:]\n",
    "\n",
    "    # set up the data \n",
    "    df['Target'] = (df['Close'].shift(-horizon) - df['Close']) / df['Close'] # set up the training target \n",
    "    X = df.drop(columns=['Close', 'Date', 'Stock', 'Target']) # drop the irrevalent variables for training set\n",
    "    y = df['Close']\n",
    "\n",
    "    # print(f\"len(X):{len(X)}, len(y):{len(y)}\")\n",
    "    # normalise the vector\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    X = scaler.fit_transform(X)\n",
    "    y = scaler.fit_transform(y.values.reshape(-1, 1))\n",
    "\n",
    "    return X, y, scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "aaecfc2e-c252-493b-97fc-841c55a958af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create sequence data for lstm\n",
    "def get_sequences2(X, y, seq_length):\n",
    "    X_seq, y_seq = [], []\n",
    "    for i in range(seq_length, len(X)):\n",
    "        X_seq.append(X[i:i+seq_length])\n",
    "        y_seq.append(y[i+seq_length])\n",
    "    return np.array(X_seq), np.array(y_seq) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "38f585ed-e9f8-47eb-8cbd-3ad8e730c8a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sequences(X, y, seq_length):\n",
    "    \"\"\"Create properly aligned sequences where:\n",
    "    - Each X sequence contains seq_length past observations\n",
    "    - Each y sequence contains corresponding targets\n",
    "    - Both have exactly the same length\"\"\"\n",
    "    X_seq, y_seq = [], []\n",
    "    for i in range(seq_length, len(X)):\n",
    "            X_seq.append(X[i-seq_length:i])  # Past seq_length observations\n",
    "            y_seq.append(y[i-seq_length:i])   # Corresponding targets\n",
    "    return np.array(X_seq), np.array(y_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "id": "0946adbe-419c-42f0-8ea0-0927c604be74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build lstm model\n",
    "def get_lstm(seq_len, num_param):\n",
    "    model = Sequential([\n",
    "        Input(shape=(seq_len, num_param)),\n",
    "        LSTM(50, return_sequences=True),\n",
    "        LSTM(50),\n",
    "        Dense(1)\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    return model\n",
    "\n",
    "def get_enhanced_lstm(seq_len, num_features):\n",
    "    model = Sequential([\n",
    "        Input(shape=(seq_len, num_features)),\n",
    "        LSTM(128, return_sequences=True, \n",
    "             kernel_regularizer=L1L2(l1=1e-5, l2=1e-4),\n",
    "             recurrent_dropout=0.2),\n",
    "        BatchNormalization(),\n",
    "        LSTM(64, return_sequences=True,\n",
    "             kernel_regularizer=L1L2(l1=1e-5, l2=1e-4)),\n",
    "        Dropout(0.3),\n",
    "        LSTM(32),\n",
    "        BatchNormalization(),\n",
    "        Dense(32, activation='relu'),\n",
    "        Dropout(0.2),\n",
    "        Dense(1)\n",
    "    ])\n",
    "    \n",
    "    optimizer = Adam(learning_rate=0.001)\n",
    "    model.compile(optimizer=optimizer, loss='mse', metrics=['mae'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# # Example usage\n",
    "# model = get_enhanced_lstm(seq_len=60, num_features=5)  # 60 timesteps, 5 features\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "id": "df86f7cf-4135-481b-a5ac-a063ea29beed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model by rolling window and predict the outcome\n",
    "def rolling_window_train(stock, horizon=1, train_ratio=0.8, window_size=720, epochs=5, batch_size=32, testing_size=1000):\n",
    "    predictions = []\n",
    "    actuals = []\n",
    "    \n",
    "    # prepare data for the seq_length\n",
    "    X, y, scaler = prepare_data(stock, seq_length=window_size, testing_size=testing_size, horizon=horizon)\n",
    "    test_start = int(len(X)*train_ratio)\n",
    "    \n",
    "    # get sequence data\n",
    "    X, y = get_sequences(X, y, window_size)\n",
    "    # print(f\"len(X_seq):{len(X_seq)}, len(y_seq):{len(y_seq)}\")\n",
    "    # print(y_seq[len(y_seq)-1], y[len(y)-1-seq_length:len(y)-1])\n",
    "\n",
    "    i = test_start\n",
    "    while (i < len(X)):\n",
    "        i_seq = i-window_size\n",
    "        # reset the model\n",
    "        model = get_enhanced_lstm(window_size, X.shape[2])\n",
    "\n",
    "        X_window, y_window = X[i_seq-window_size:i_seq], y[i_seq-window_size:i_seq]\n",
    "        steps = window_size // batch_size // epochs\n",
    "        if (i == test_start):\n",
    "            print(len(X_window), window_size, steps*epochs*batch_size)\n",
    "\n",
    "        # fit the model with rolling window\n",
    "        model.fit(X_window, y_window, epochs=epochs, batch_size=batch_size, steps_per_epoch=steps, verbose=0)\n",
    "\n",
    "        # Predict next value\n",
    "        pred = model.predict(X[i_seq:i_seq+1].reshape(1, window_size, X.shape[2]))\n",
    "        pred = scaler.inverse_transform(pred[0,0].reshape(-1,1))\n",
    "        predictions.append(pred)\n",
    "        y_test_original = scaler.inverse_transform(y[i_seq][0].reshape(-1,1))\n",
    "        actuals.append(y_test_original)\n",
    "\n",
    "        i += 1\n",
    "\n",
    "    return np.array(predictions).flatten(), np.array(actuals).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "id": "fa30fa3c-0072-4bd9-8a2d-8c196e325213",
   "metadata": {},
   "outputs": [],
   "source": [
    "def expanding_window_train(stock, horizon, train_ratio=0.8, window_size=720, epochs=5, batch_size=32, testing_size=1000):\n",
    "    predictions = []\n",
    "    actuals = []\n",
    "    X, y, scaler = prepare_data(stock, seq_length=window_size, testing_size=testing_size, horizon=horizon)\n",
    "    test_start = int(len(X)*train_ratio)\n",
    "\n",
    "    i = test_start\n",
    "    while (i < len(X)):\n",
    "        i_seq = i-window_size\n",
    "        X_seq, y_seq = get_sequences(X, y, window_size)\n",
    "        # reset the model\n",
    "        model = get_enhanced_lstm(window_size, X_seq.shape[2])\n",
    "\n",
    "        X_window, y_window = X_seq[i_seq-window_size:i_seq], y_seq[i_seq-window_size:i_seq]\n",
    "        steps = window_size // batch_size // epochs\n",
    "        if (i == test_start):\n",
    "            print(len(X_window), window_size, steps*epochs*batch_size)\n",
    "\n",
    "        # fit the model with rolling window\n",
    "        model.fit(X_window, y_window, epochs=epochs, batch_size=batch_size, steps_per_epoch=steps, verbose=0)\n",
    "\n",
    "        # Predict next value\n",
    "        print(X_seq[i_seq:i_seq+1].shape)\n",
    "        pred = model.predict(X_seq[i_seq:i_seq+1].reshape(1, window_size, X_seq.shape[2]))\n",
    "        pred = scaler.inverse_transform(pred[0,0].reshape(-1,1))\n",
    "        predictions.append(pred)\n",
    "        y_test_original = scaler.inverse_transform(y_seq[i_seq][0].reshape(-1,1))\n",
    "        actuals.append(y_test_original)\n",
    "\n",
    "        i += 1\n",
    "        window_size += 1\n",
    "\n",
    "    return np.array(predictions).flatten(), np.array(actuals).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "id": "1bba318e-f080-42d6-906f-1201b3b3aa44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # predictions, actuals = rolling_window_train(STOCKS[0], HORIZONS[0], window_size=10, epochs=2, batch_size=5, testing_size=100)\n",
    "    predictions, actuals = expanding_window_train(STOCKS[0], HORIZONS[0], window_size=10, epochs=2, batch_size=5, testing_size=100)\n",
    "    # evaulate the model performance\n",
    "    print(predictions - actuals)\n",
    "    rmse = np.sqrt(mean_squared_error(actuals.reshape(-1, 1), predictions.reshape(-1, 1)))\n",
    "    print(f'RMSE: {rmse}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "id": "37fb6da5-4ff9-4e0c-8ec6-1d20f1621651",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    X, y, scaler = prepare_data(STOCKS[0], horizon=HORIZONS[0], seq_length=730, train_ratio=0.8)\n",
    "    print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af12fba0-7189-401c-a966-173981d3330b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â Processed data already exists. Skipping processing.\n",
      "â Technical indicators added successfully.\n",
      "10 10 10\n",
      "(1, 10, 22)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mââââââââââââââââââââ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 695ms/step\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    main()\n",
    "    # test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "226fb1b3-b9d4-45cd-a404-9f56e25610eb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
